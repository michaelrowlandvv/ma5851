{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOl7FPnn3b5BqBjUn2Z2qpW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**ABC News Webcrawler**\n","\n","Import dependencies"],"metadata":{"id":"u5O8gJiPORaC"}},{"cell_type":"code","source":["import nltk\n","import pandas as pd\n","import numpy as np\n","import json\n","from urllib.request import Request, urlopen\n","import re\n","from bs4 import BeautifulSoup\n","import matplotlib.pyplot as plt \n","from nltk.stem.porter import PorterStemmer\n","import seaborn as sns\n","nltk.download('stopwords')\n","from sklearn.model_selection import train_test_split\n","from nltk.corpus import stopwords\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PZPkIxk7Qpyf","executionInfo":{"status":"ok","timestamp":1682466325571,"user_tz":-600,"elapsed":27764,"user":{"displayName":"Hey Boo","userId":"02448068063730063578"}},"outputId":"ffd40025-1385-4b24-d9a5-0eb6665dab21"},"execution_count":105,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["Helper functions"],"metadata":{"id":"eb1VwKMOQQch"}},{"cell_type":"code","source":["#frequency of words\n","def freq_words(x, terms = 30): \n","  all_words = ' '.join([text for text in x]) \n","  all_words = all_words.split() \n","  fdist = nltk.FreqDist(all_words) \n","  words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())}) \n","  \n","  # selecting top n most frequent words \n","  d = words_df.nlargest(columns=\"count\", n = terms) \n","  # visualize words and frequencies\n","  plt.figure(figsize=(12,15)) \n","  ax = sns.barplot(data=d, x= \"count\", y = \"word\") \n","  ax.set(ylabel = 'Word') \n","  ax.set_xlabel('Count', fontsize = 46)\n","  ax.set_ylabel('Word', fontsize = 46)\n","  ax.tick_params(axis='both', which='major', labelsize=46)\n","  plt.show()\n","  \n","#determines unique words\n","def uniqueWords(x):\n","  words = ' '.join([text for text in x]) \n","  words = nltk.tokenize.word_tokenize(words)\n","  fdist1 = nltk.FreqDist(words)\n","  freq = dict((word, freq) for word, freq in fdist1.items() if not word.isdigit())\n","  return freq"],"metadata":{"id":"z3Jxr1usQVJR","executionInfo":{"status":"ok","timestamp":1682457369342,"user_tz":-600,"elapsed":4,"user":{"displayName":"Hey Boo","userId":"02448068063730063578"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Generate lists of sites from archive"],"metadata":{"id":"Eb0ojnv5QsTc"}},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R_j2TDm1OEvr","executionInfo":{"status":"ok","timestamp":1682457375588,"user_tz":-600,"elapsed":1203,"user":{"displayName":"Hey Boo","userId":"02448068063730063578"}},"outputId":"3316455d-1208-4dd0-cf00-df7587de9e48"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["66304"]},"metadata":{},"execution_count":4}],"source":["import urllib.request\n","page = urllib.request.urlopen('http://web.archive.org/cdx/search/cdx?url=abc.net.au/news/')\n","contents= page.read()\n","contents \n","stringcontents = contents.decode(\"utf-8\")\n","stringcontents = stringcontents.splitlines()\n","len(stringcontents) #66167 weblinks"]},{"cell_type":"markdown","source":["Generate a dictionary containing the eight digit date as key and the full website encode for navigation to archived page"],"metadata":{"id":"mysuce_pQG8d"}},{"cell_type":"code","source":["contentssplit = contents.splitlines()\n","len(contentssplit)\n","dictPages = {}\n","for c in contentssplit:\n","  stringc = c.decode(\"utf-8\") \n","  csplit=stringc.split()\n","  for cc in csplit:\n","    cc=str(cc)\n","    print(cc)\n","    if cc.startswith(\"19\") | cc.startswith(\"20\"):\n","      testkey = int(cc[:8])  #10210101\n","      print (\"test\", testkey)\n","      if testkey >= 20210101:\n","        if not testkey in dictPages.keys():\n","          dictPages[testkey] = cc\n","      break\n","len(dictPages.keys()) #845 days"],"metadata":{"id":"YxS2SmqoQF4N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Extraction of content from news and story pages using BeautifulSoup"],"metadata":{"id":"BqBwhnb-UW7H"}},{"cell_type":"code","source":["root = \"http://www.abc.net/news/\"\n","keyterms=[\"republic-\", \"republic \", \"constitution\", \"monarch\",]\n","articleurls=[] #contains article already scanned\n","#news page: returns a list of synopses and call for article text\n","def extractcontentfromnewspage(link):\n","    req = Request(link, headers={'User-Agent': 'Mozilla/5.0'})\n","    output=[]\n","    webpage = urlopen(req).read()\n","    soup = BeautifulSoup(webpage, 'html5lib')\n","    synopses = soup.findAll(\"div\", {\"data-component\" : \"CardDescription\"})\n","    for synopsis in synopses:\n","      #add a synopsis to output if relevant ref keyterms\n","      found = False \n","      if not found:\n","        for term in keyterms:\n","          if term in synopsis.get_text().lower():\n","            output.append(synopsis.get_text())\n","            found = True\n","            break\n","    # find the article pages\n","    anchors = soup.findAll(\"a\", {\"class\" : \"VolumeCard_link__GOaqC\"})\n","    for a in soup.find_all('a', href=True):\n","      #if a news site and not already crawled\n","      if 'abc.net.au/news' in a and not a in articleurls:\n","        url = a['href']\n","        articleurls.append(url)\n","        found = False\n","        for term in keyterms:\n","          if not found:\n","            if term in url.lower():\n","              found = True\n","              articletext = extractcontentfromarticlepage(url)\n","              if articletext:\n","                output.append(articletext)\n","              break\n","    return output\n","\n","#extracts info from article page\n","def extractcontentfromarticlepage(link):\n","  req = Request(link, headers={'User-Agent': 'Mozilla/5.0'})\n","  webpage = urlopen(req).read()\n","  soup = BeautifulSoup(webpage, 'html5lib')\n","  articletext=''\n","  paragraphs = soup.findAll(\"p\", {\"class\" : \"paragraph_paragraph__3Hrfa\"})\n","  for paragraph in paragraphs:\n","    articletext += paragraph.get_text()\n","  return articletext"],"metadata":{"id":"ol6-wF-yUI2x","executionInfo":{"status":"ok","timestamp":1682464338904,"user_tz":-600,"elapsed":591,"user":{"displayName":"Hey Boo","userId":"02448068063730063578"}}},"execution_count":94,"outputs":[]},{"cell_type":"code","source":["dictContent={}"],"metadata":{"id":"K5Ubc7c_pUPJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Constuct a link and call page function to extract headline and summary info"],"metadata":{"id":"XgtdvbCUTY2i"}},{"cell_type":"code","source":["\n","from bs4 import BeautifulSoup\n","i=0\n","startindex=828\n","\n","#used for resuming after error\n","for key, value in dictPages.items():\n","  i=i+1\n","  if i>= startindex:\n","    url = \"https://web.archive.org/web/\" + str(value) + \"/http://abc.net.au/news\"\n","    print(i)\n","    output = extractcontentfromnewspage(url)\n","    if output:\n","      dictContent[key] = []\n","      for content in output:\n","        dictContent[key] += [content]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EtQdHpl_R5uH","executionInfo":{"status":"ok","timestamp":1682475119305,"user_tz":-600,"elapsed":264300,"user":{"displayName":"Hey Boo","userId":"02448068063730063578"}},"outputId":"9c4f24ff-0a2b-4953-f244-ac2904b826dd"},"execution_count":128,"outputs":[{"output_type":"stream","name":"stdout","text":["828\n","829\n","830\n","831\n","832\n","833\n","834\n","835\n","836\n","837\n","838\n","839\n","840\n","841\n","842\n","843\n","844\n","845\n"]}]},{"cell_type":"code","source":["np.save(\"/content/drive/MyDrive/A3/ABCbackupFinal.npy\", dictContent)"],"metadata":{"id":"0aaLC-HPh6ml","executionInfo":{"status":"ok","timestamp":1682475129243,"user_tz":-600,"elapsed":424,"user":{"displayName":"Hey Boo","userId":"02448068063730063578"}}},"execution_count":129,"outputs":[]},{"cell_type":"code","source":["dictContent.keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bTz4KQxFhDTi","executionInfo":{"status":"ok","timestamp":1682468380480,"user_tz":-600,"elapsed":405,"user":{"displayName":"Hey Boo","userId":"02448068063730063578"}},"outputId":"2396f636-c155-4914-c02f-2ff17c362089"},"execution_count":113,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys([20210107, 20210115, 20210116, 20210119, 20210120, 20210127, 20210209, 20210210, 20210313, 20210314, 20210321, 20210513, 20210610, 20210615, 20210626, 20210819, 20210831, 20210922])"]},"metadata":{},"execution_count":113}]}]}